# -*- coding: utf-8 -*-
"""ML3 - TITANIC_ Kaggle competition.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1p_0aKbsejRQxAUPK-CA0HrALq9vM28FK

# TITANIC: Kaggle competition

**TAREA: CLASIFICACIÓN**<br>
**NIVEL: INTERMEDIATE**

Vamos a participar en la competicion de **Kaggle del Titanic**: https://www.kaggle.com/competitions/titanic <br> Para ello atacaremos 3 puntos clave para obtener el mejor modelo posible:
* ML Preprocessing
* Competicion de modelos
* Hiperparametros 
<br><br>
A diferencia de en el flujo ML convencional, en las competiciones de Kaggle no haremos la evaluación del TEST en el notebook, sino que tendremos que subir un predict del test a la plataforma de submissions y la plataforma lo evaluará.
<br><br>
...A POR ELLO!

# Libraries
"""

import pandas as pd     # Manejo de dataframes
import numpy as np      # Calculos matriciales
import matplotlib.pyplot as plt    # Visualizacion
import seaborn as sns              # Visualizacion
from google.colab import drive     # Drive en Colab

pd.set_option('max.columns',100)   # para visualizar mas columnas en un df

import warnings
warnings.filterwarnings("ignore")  # inhabilita warnings

"""# Load dataset"""

from google.colab import drive
drive.mount('/content/drive')

titanic = pd.read_csv('/content/drive/MyDrive/Data science/Machine learning/ML3 - Kaggle competition/titanic_train.csv')

print(titanic.shape)

titanic.head(2)

titanic.describe().T

titanic.info()

"""# ML Preprocessing"""

titanic2 = titanic.copy()

titanic2[titanic2.duplicated()]

titanic2.isna().sum()

titanic2['Age'].value_counts().sum() / titanic2.shape[0] * 100

titanic2['Age'].isna().sum() / titanic2.shape[0] * 100

titanic2['Cabin'].isna().sum() / titanic2.shape[0] * 100

titanic2['Cabin'] = titanic2['Cabin'].fillna('Unknown')

titanic2['Embarked'].isna().sum() / titanic2.shape[0] * 100

titanic2['Age'] = titanic2['Age'].fillna(29)                      # imputamos media en Edad
titanic2['Embarked'] = titanic2['Embarked'].fillna('S')

titanic2.isna().sum()

"""* Data cleaning"""

#Set index
titanic2.set_index('PassengerId', inplace=True)

titanic2.info()

#Checking types of columns and changing when required
titanic3 = titanic2.copy()

titanic3.info()

titanic3['Survived'].value_counts()

titanic3['Pclass'].value_counts()

titanic3['Sex'].value_counts()

titanic3['SibSp'].value_counts()

titanic3['Parch'].value_counts()

titanic3['Embarked'].value_counts()

#Listing categorical columns
titanic4 = titanic3.copy()
categories = list(titanic4.select_dtypes(include='object').columns)

pd.set_option('max.rows',50)

for i in categories:
  print(i)
  print('Unique:',titanic4[i].nunique())
  print(titanic4[i].value_counts(),'\n')

titanic4.info()

# Drop columns that lead to overfitting (that is, a lack of generalization)
titanic4.drop(columns=['Name','Ticket'], inplace=True)

# Leaving the letter of the cabin
titanic4['Cabin_letter'] = titanic4['Cabin'].str[0]
titanic4.drop(columns='Cabin', inplace=True)

# Data types
titanic4.info()

"""* One Hot Encoder"""

titanic5 = pd.get_dummies(titanic4)

titanic5.head(2)

corr = titanic5.corr()

corr.style.background_gradient(cmap = 'coolwarm')

del(titanic5['Sex_male'])

"""# Split X-y"""

target = 'Survived'
features = titanic5.columns[titanic5.columns != target]

y = titanic5[target]
X = titanic5[features]

print('Target del modelo:', target)
print('Variables a utilizar en el modelo:', list(features))

"""**---------------------------------------------------------------------<br>
A partir de aqui el ejercicio esta por completar. Aplica lo visto en anteriores ejercicios para completar los siguientes pasos y ganar la competición <br>    ---------------------------------------------------------------------**

# Target distribution
"""

y.value_counts() / titanic5.shape[0] * 100

sns.countplot(y)
plt.title('TARGET DISTRIBUTION: '+target)
plt.show()

"""# Split train-test (NO)

Como estamos en una competicion no sacrificaremos parte del dataset para TEST, haremos Cross-Validation para seleccionar el mejor modelo.
"""

X_train = X
y_train = y

"""# Rescaling

Estandarizamos en el train data
"""

from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X_train_std = scaler.fit_transform(X_train)
X_train_std = pd.DataFrame(data = X_train_std, columns=X_train.columns, index=X_train.index)

X_train_std.head(2)

"""# TRAIN MODEL

## Model competition

Prueba distintos modelos y mide su performance con Cross Validation
"""

from sklearn.model_selection import cross_val_score
from sklearn.metrics import roc_auc_score

# Calculo del AUC de un modelo sobre un dataset (X-y). Args: modelo a validar, X, y
def auc(model,X,y):
  model.fit(X,y)
  return roc_auc_score(y,model.predict_proba(X)[:,1])  

# Calculo del AUC de un modelo sobre un dataset con Cross-Validation (X-y). Args: modelo a validar, X, y  
def auc_cross_validation(model,X,y):
  cross_val = cross_val_score(model, X, y, cv=5, scoring='roc_auc')    # Definimos cuantos KFolds (cv) y la metrica validacion (scoring)
  return cross_val.mean()

from sklearn.linear_model import LogisticRegression
print('AUC en TRAIN:', auc(LogisticRegression(),X_train_std,y_train) )
print('AUC en CROSS VALIDATION:', auc_cross_validation(LogisticRegression(),X_train_std,y_train) )

# Inicializa resultados
label_, auc_train_ , auc_valida_ = [], [], []

# Completa un registro en dataframe con los resultados de las funciones definidas previsamente ( AUC y AUC con cross-validation)
def evaluate_classification(label, model, X, y):
  auc_train = auc(model,X,y)
  auc_valida = auc_cross_validation(model,X,y)                  
  label_.append(label)
  auc_train_.append(auc_train)
  auc_valida_.append(auc_valida)
  return pd.DataFrame({'Model': label_,
                      'AUC Train': auc_train_,
                      'AUC Cross-Valida': auc_valida_
                      }).sort_values('AUC Cross-Valida',ascending=False)

from sklearn.linear_model import LogisticRegression

evaluate_classification(label='Logistic Regression', 
                        model=LogisticRegression(), 
                        X=X_train_std, y=y_train)

from sklearn.ensemble import AdaBoostClassifier, BaggingClassifier, GradientBoostingClassifier, RandomForestClassifier, HistGradientBoostingClassifier

evaluate_classification(label='AdaBoost', 
                        model=AdaBoostClassifier(), 
                        X=X_train_std, y=y_train)

evaluate_classification(label='Bagging', 
                        model=BaggingClassifier(), 
                        X=X_train_std, y=y_train)

evaluate_classification(label='GradientBoosting', 
                        model=GradientBoostingClassifier(), 
                        X=X_train_std, y=y_train)

evaluate_classification(label='RandomForest', 
                        model=RandomForestClassifier(), 
                        X=X_train_std, y=y_train)

evaluate_classification(label='HistGradientBoosting', 
                        model=HistGradientBoostingClassifier(), 
                        X=X_train_std, y=y_train)

from xgboost import XGBClassifier

evaluate_classification(label='XGBoost', 
                        model=XGBClassifier(), 
                        X=X_train_std, y=y_train)

from sklearn.svm import SVC

evaluate_classification(label='Support Vector Machine (kernel=rbf)', 
                        model=SVC(probability=True), 
                        X=X_train_std, y=y_train)

"""## Hyperparameter tuning"""

from sklearn.model_selection import RandomizedSearchCV

parameters_hp = {
    'loss': ['log_loss', 'deviance'], 
    'learning_rate': [0.01,0.1,0.3], 
    'n_estimators': [50,100,500,1000], 
    'subsample': [0.5,0.7,1],
    'max_depth': [3,4,5,6,7]
}

search_hp = RandomizedSearchCV(estimator = GradientBoostingClassifier(), 
                               param_distributions = parameters_hp,
                               scoring = 'roc_auc',                         # metrica a optimizar: AUC
                               cv = 4,                                      # numero de folds para cross-validation
                               n_iter = 20,                                 # numero de iteraciones, cada una testeara una configuracion distinta
                               n_jobs=-1, 
                               verbose=True)

search_hp.fit(X_train_std,y_train)

print("---- Results from Random Search -----")
print("\n The best estimator across ALL searched params:", search_hp.best_estimator_)
print("\n The best AUC:", search_hp.best_score_)
print("\n The best parameters across ALL searched params:\n", search_hp.best_params_)

"""## Final model

Como hemos aplicado hiperparametrizacion, el modelo sera el resultado del proceso.
"""

choosen_model = search_hp.best_estimator_

"""# TEST PREDICTIONS

Para hacer el submit de los datos en la competicion, tenemos que hacer un predict sobre el dataset 'titanic_test.csv'.
"""

X_pred = pd.read_csv('/content/drive/MyDrive/Data science/Machine learning/ML3 - Kaggle competition/titanic_test.csv')
X_pred.head()

#Para recuperar variables que se han eliminado
features_model =  list(choosen_model.feature_names_in_)
print('Estas son las variables que tenemos que conseguir en el preprocessing para que el modelo funcione:\n', features_model)

"""## ML Preprocessing

Al igual que en el preprocessing inicial, repetimos:
* Drop de columnas
* Imputación de nulos (mismos valores)
* One Hot Encoding
"""

# Drop variables con un valor unico por instancia que favorecen el overfitting (falta de generalización) como el Nombre o el Numero de ticket
X_pred.drop(columns=['Name','Ticket'], inplace=True)

# Nos quedamos solo con la letra de la cabina
X_pred['Cabin_letter'] = X_pred['Cabin'].str[0]
X_pred.drop(columns='Cabin', inplace=True)

X_pred.isna().sum()

X_pred['Cabin_letter'] = X_pred['Cabin_letter'].fillna('Unknown')

X_pred['Age'] = X_pred['Age'].fillna(29)                     
X_pred['Embarked'] = X_pred['Embarked'].fillna('S')
X_pred['Fare'] = X_pred['Fare'].fillna(32.2)

#Set index
X_pred.set_index('PassengerId', inplace=True)

titanic5.info()

X_pred.info()

categories = ['Sex', 'Embarked', 'Cabin_letter']

X_pred = pd.get_dummies(X_pred, columns=categories)

X_pred.head(2)

titanic5.head(2)

del(X_pred['Sex_male'])

features_test = list(X_pred.columns)

print('Predict features:',len(features_test))
print('Model features:',len(features_model))
print('Match:', features_model == features_test)

missing_features = [i for i in features_model if i not in features_test]
print(len(missing_features),'features missing in the test data:\n', missing_features)

drop_features = [i for i in features_test if i not in features_model]
print(len(drop_features),'features you should drop in the test data:\n', drop_features)

for col in missing_features:
  X_pred[col]= 0

features_test = list(X_pred.columns)

print('Predict features:',len(features_test))
print('Model features:',len(features_model))
print('Match:', features_model == features_test)

del(X_pred['Cabin_letter_Unknown'])

titanic5.info()

X_pred.info()

X_pred = X_pred[features_model]

"""## Rescaling

Aplicamos mismo scaler que en el training (".transform", no hacer ".fit")
"""

X_pred_std = scaler.transform(X_pred)
X_pred_std = pd.DataFrame(data = X_pred_std, columns=X_pred.columns, index=X_pred.index)

"""## Predictions"""

predictions = choosen_model.predict(X_pred_std)

predictions

"""# Upload predictions to Kaggle.com

Se deben subir las predicciones en un .csv con 2 columnas (PassengerId, Survived) en el apartado de submissions en https://www.kaggle.com/competitions/titanic/data. Debe contener 418 rows + header.
"""

submission = pd.DataFrame(predictions, columns=['Survived'], index=X_pred.index)

submission.to_csv('/content/drive/MyDrive/Data science/Machine learning/ML3 - Kaggle competition/3.csv')